## <u>Technical Skills</u> <sup>[⎙ CV](https://docs.google.com/document/d/1_SPIS3m1oue7XEYCdj2fCFJC9JQ9D37jNmETILRxuI0/export?format=pdf)</sup>

**Programming & Frameworks**: Python, SQL, Django, FastAPI <br>
**Data Engineering**: ETL Development, Data Modelling, Data Warehousing, CDC, Data Quality <br>
**Big Data & Orchestration**: Apache Spark, Airflow, Kafka; dbt, <br>
**Databases**: PostgreSQL, MySQL, MongoDB, Cassandra <br>
**Cloud & DevOps**: AWS (EC2, S3, Lambda, Step Functions), Docker, Git <br>

## <u>Work Experience</u>

**Data Collection & Analysis Coordinator​ @ Pro-Team Support Services (*March 2022 - Present*)**

- Oversaw end-to-end data processes (collection, transformation, reporting) supporting humanitarian NGO projects.
- Migrated workflows to digital tools like KoboCollect, enabling streamlined data ingestion and validation.
- Designed and implemented lightweight ETL pipelines in Python and Pandas to clean, transform, and structure raw survey data.
- Generated dashboards and analytics using Excel automating insight delivery.


## <u> Education</u>

- Bsc. Computer Science - Africa Nazarene University​ (*July 2022*)

## <u>Projects</u>
**[Cars&Bids ETL Pipeline](https://github.com/BrianOyollo/Cars-Bids-Data-Pipeline-v0-)** | *Python, Selenium, Pandas, PostgreSQL, AWS(ec2,s3,lambda functions, step functions)*

- Engineered an automated pipeline to scrape, clean, and load car auction data daily into an AWS-based data warehouse.
- Built transformation logic in Pandas for schema standardization and enrichment before S3 storage.
- Deployed AWS Lambda and Step Functions for serverless orchestration and scalable data processing.
- Modeled data for analytical queries using PostgreSQL and designed a warehouse schema to support historical trend analysis.


**[Real-Time Air Quality Data Pipeline](https://github.com/BrianOyollo/air-quality-cdc)** | *Python, Apache Kafka, MongoDB, Cassandra, Docker*

- Developed a real-time streaming pipeline ingesting environmental metrics via Open-Meteo API.
- Implemented CDC between MongoDB (raw) and Cassandra (analytics) using Kafka for near-real-time sync.
- Containerized services using Docker for reproducibility and scalability.
- Optimized query performance for historical air quality trends using Cassandra’s time-series design.



## <u>Contacts</u>
Email: [oyollobrian@gmail.com](mailto:oyollobrian@gmail.com) <br>
GitHub: [github.com/BrianOyollo](https://github.com/BrianOyollo) <br>
LinkedIn: [linkedin.com/brian-oyollo](https://www.linkedin.com/in/brian-oyollo-95848019b/)
